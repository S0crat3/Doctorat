{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense,\n",
    "    Reshape,\n",
    "    UpSampling2D,\n",
    "    Conv2D,\n",
    "    BatchNormalization,\n",
    "    Add,\n",
    "    Layer\n",
    ")\n",
    "from tensorflow.keras.layers import (\n",
    "    LeakyReLU,\n",
    "    Dropout,\n",
    "    Flatten,\n",
    "    Activation,\n",
    ")\n",
    "import warnings\n",
    "\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed for reproducibility\n",
    "SEED = 1618\n",
    "\n",
    "# Set Python's built-in random seed\n",
    "random.seed(SEED)\n",
    "\n",
    "# Set NumPy random seed\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Set TensorFlow random seed\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Make sure any new random state is also deterministic\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories\n",
    "INPUT_DIR = \"../../data_resized\"\n",
    "now_fmt = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "OUTPUT_DIR = f\"../../data_generated/{now_fmt}\"\n",
    "os.mkdir(OUTPUT_DIR)\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "IMG_HEIGHT = 256\n",
    "IMG_WIDTH = 256\n",
    "EPOCHS = 200\n",
    "LATENT_DIM = 128\n",
    "NUM_EXAMPLES_TO_GENERATE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.keras.utils.image_dataset_from_directory(\n",
    "    INPUT_DIR,\n",
    "    labels=None,  # Set to None as you're not using labels\n",
    "    label_mode=None,\n",
    "    image_size=(IMG_WIDTH, IMG_HEIGHT),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,  # Enable shuffling for better training\n",
    ")\n",
    "\n",
    "# Apply rescaling\n",
    "data = data.map(lambda x: x / 127.5 - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom residual block as a layer\n",
    "class ResidualBlock(Layer):\n",
    "    def __init__(self, filters, kernel_size=3):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = Conv2D(filters, kernel_size=kernel_size, padding=\"same\")\n",
    "        self.batchnorm1 = BatchNormalization()\n",
    "        self.activation1 = Activation(\"relu\")\n",
    "        self.conv2 = Conv2D(filters, kernel_size=kernel_size, padding=\"same\")\n",
    "        self.batchnorm2 = BatchNormalization()\n",
    "        self.activation2 = Activation(\"relu\")\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.batchnorm1(x, training=training)\n",
    "        x = self.activation1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.batchnorm2(x, training=training)\n",
    "        x = Add()([inputs, x])\n",
    "        return self.activation2(x)\n",
    "\n",
    "generator = Sequential()\n",
    "\n",
    "# First layer: fully connected\n",
    "generator.add(Dense(8 * 8 * 512, input_dim=LATENT_DIM))\n",
    "generator.add(Reshape((8, 8, 512)))\n",
    "generator.add(BatchNormalization())\n",
    "generator.add(Activation(\"relu\"))\n",
    "\n",
    "# First UpSampling Block: Upsample to 16x16\n",
    "generator.add(UpSampling2D(size=(2, 2)))  # Use UpSampling instead of Conv2DTranspose\n",
    "generator.add(Conv2D(256, kernel_size=5, padding=\"same\"))  # Convolution after upsampling\n",
    "generator.add(BatchNormalization())\n",
    "generator.add(Activation(\"relu\"))\n",
    "\n",
    "\n",
    "# Second UpSampling Block: Upsample to 32x32\n",
    "generator.add(UpSampling2D(size=(2, 2)))\n",
    "generator.add(Conv2D(128, kernel_size=5, padding=\"same\"))\n",
    "generator.add(BatchNormalization())\n",
    "generator.add(Activation(\"relu\"))\n",
    "\n",
    "# Third UpSampling Block: Upsample to 64x64\n",
    "generator.add(UpSampling2D(size=(2, 2)))\n",
    "generator.add(Conv2D(64, kernel_size=5, padding=\"same\"))\n",
    "generator.add(BatchNormalization())\n",
    "generator.add(Activation(\"relu\"))\n",
    "\n",
    "# Fourth UpSampling Block: Upsample to 128x128\n",
    "generator.add(UpSampling2D(size=(2, 2)))\n",
    "generator.add(Conv2D(32, kernel_size=5, padding=\"same\"))\n",
    "generator.add(BatchNormalization())\n",
    "generator.add(Activation(\"relu\"))\n",
    "\n",
    "# Final UpSampling Block: Upsample to 256x256\n",
    "generator.add(UpSampling2D(size=(2, 2)))\n",
    "generator.add(Conv2D(16, kernel_size=5, padding=\"same\"))\n",
    "generator.add(BatchNormalization())\n",
    "generator.add(Activation(\"relu\"))\n",
    "\n",
    "# Final layer: Convolution to generate the 256x256x3 image\n",
    "generator.add(Conv2D(3, kernel_size=5, padding=\"same\"))\n",
    "generator.add(Activation(\"tanh\"))  # Output values are in the range [-1, 1]\n",
    "\n",
    "generator.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = Sequential()\n",
    "\n",
    "# First Convolutional Block: Downsample to 128x128\n",
    "discriminator.add(Conv2D(64, kernel_size=5, strides=2, padding=\"same\", input_shape=(IMG_WIDTH, IMG_HEIGHT, 3)))\n",
    "discriminator.add(LeakyReLU(alpha=0.2))\n",
    "discriminator.add(Dropout(0.3))\n",
    "\n",
    "# Second Convolutional Block: Downsample to 64x64\n",
    "discriminator.add(Conv2D(128, kernel_size=5, strides=2, padding=\"same\"))\n",
    "discriminator.add(BatchNormalization())\n",
    "discriminator.add(LeakyReLU(alpha=0.2))\n",
    "discriminator.add(Dropout(0.3))\n",
    "\n",
    "# Third Convolutional Block: Downsample to 32x32\n",
    "discriminator.add(Conv2D(256, kernel_size=5, strides=2, padding=\"same\"))\n",
    "discriminator.add(BatchNormalization())\n",
    "discriminator.add(LeakyReLU(alpha=0.2))\n",
    "discriminator.add(Dropout(0.3))\n",
    "\n",
    "# Fourth Convolutional Block: Downsample to 16x16\n",
    "discriminator.add(Conv2D(512, kernel_size=5, strides=2, padding=\"same\"))\n",
    "discriminator.add(BatchNormalization())\n",
    "discriminator.add(LeakyReLU(alpha=0.2))\n",
    "discriminator.add(Dropout(0.3))\n",
    "\n",
    "# # Fifth Convolutional Block (New): Downsample to 8x8\n",
    "discriminator.add(Conv2D(512, kernel_size=5, strides=2, padding=\"same\"))\n",
    "discriminator.add(BatchNormalization())\n",
    "discriminator.add(LeakyReLU(alpha=0.2))\n",
    "discriminator.add(Dropout(0.3))\n",
    "\n",
    "# Flattening and Output\n",
    "discriminator.add(Flatten())\n",
    "discriminator.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "# Display model summary\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(tf.keras.Model):\n",
    "    def __init__(self, discriminator, generator, latent_dim):\n",
    "        super(GAN, self).__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
    "        super(GAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.d_loss_metric = tf.keras.metrics.Mean(name=\"d_loss\")\n",
    "        self.g_loss_metric = tf.keras.metrics.Mean(name=\"g_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.d_loss_metric, self.g_loss_metric]\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, real_images):\n",
    "        # Sample random points in the latent space\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        seed = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        # Decode them to fake images\n",
    "        generated_images = self.generator(seed)\n",
    "        # Combine them with real images\n",
    "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
    "\n",
    "        # Assemble labels discriminating real from fake images\n",
    "        labels = tf.concat(\n",
    "            [tf.zeros((batch_size, 1)), tf.ones((batch_size, 1)) * 0.9], axis=0\n",
    "        )\n",
    "\n",
    "        # Train the discriminator\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(combined_images)\n",
    "            d_loss = self.loss_fn(labels, predictions)\n",
    "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "        self.d_optimizer.apply_gradients(\n",
    "            zip(grads, self.discriminator.trainable_weights)\n",
    "        )\n",
    "\n",
    "        # Sample random points in the latent space\n",
    "        seed = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "\n",
    "        # Assemble labels that say \"all real images\"\n",
    "        misleading_labels = tf.ones((batch_size, 1))\n",
    "\n",
    "        # Train the generator (note that we should *not* update the weights of the discriminator)!\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(self.generator(seed))\n",
    "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
    "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
    "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
    "\n",
    "        # Update metrics\n",
    "        self.d_loss_metric.update_state(d_loss)\n",
    "        self.g_loss_metric.update_state(g_loss)\n",
    "        return {\n",
    "            \"d_loss\": self.d_loss_metric.result(),\n",
    "            \"g_loss\": self.g_loss_metric.result(),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveImageCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        seed = tf.random.normal(shape=(NUM_EXAMPLES_TO_GENERATE, LATENT_DIM))\n",
    "        predictions = self.model.generator(seed, training=False)\n",
    "        fig = plt.figure(figsize=(10, 10))\n",
    "        for i in range(predictions.shape[0]):\n",
    "            plt.subplot(4, 4, i + 1)\n",
    "            plt.imshow(predictions[i] * 0.5 + 0.5)\n",
    "            plt.axis(\"off\")\n",
    "        plt.savefig(f\"{OUTPUT_DIR}/image_at_epoch_{epoch:04d}.png\")\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The optimizers for Generator and Discriminator\n",
    "discriminator_opt = tf.keras.optimizers.legacy.Adam(learning_rate=5e-5, beta_1=0.5, clipnorm=1.0)\n",
    "generator_opt = tf.keras.optimizers.legacy.Adam(learning_rate=1e-4, beta_1=0.5, clipnorm=1.0)\n",
    "# To compute cross entropy loss\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "\n",
    "# Defining GAN Model\n",
    "model = GAN(discriminator=discriminator, generator=generator, latent_dim=LATENT_DIM)\n",
    "\n",
    "# Compiling GAN Model\n",
    "model.compile(d_optimizer=discriminator_opt, g_optimizer=generator_opt, loss_fn=loss_fn)\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "# Fitting the GAN\n",
    "history = model.fit(data, epochs=EPOCHS, callbacks=[SaveImageCallback(), tensorboard_callback])\n",
    "\n",
    "model.save(f\"{OUTPUT_DIR}/model_final.keras\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
